{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [写経] TensorFlowを使った機械学習ことはじめ\n",
    "- [TensorFlow を使った 機械学習ことはじめ (GDG京都 機械学習勉強会)](https://www.slideshare.net/ToruUenoyama/tensorflow-gdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気まぐれおやじのこころを読む Ⅰ\n",
    "9ページ～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "訓練データ\n",
      "x= [[1. 3.]\n",
      " [3. 1.]\n",
      " [5. 7.]]\n",
      "y= [[190.]\n",
      " [330.]\n",
      " [660.]]\n",
      "\n",
      "推定パラメータの推移\n",
      "step= 10, a1= 70.35, a2= 46.23, loss=2189.06\n",
      "step= 20, a1= 83.06, a2= 36.70, loss=771.90\n",
      "step= 30, a1= 90.13, a2= 31.41, loss=334.34\n",
      "step= 40, a1= 94.05, a2= 28.47, loss=199.24\n",
      "step= 50, a1= 96.23, a2= 26.84, loss=157.52\n",
      "step= 60, a1= 97.44, a2= 25.93, loss=144.64\n",
      "step= 70, a1= 98.12, a2= 25.42, loss=140.67\n",
      "step= 80, a1= 98.49, a2= 25.14, loss=139.44\n",
      "step= 90, a1= 98.70, a2= 24.99, loss=139.06\n",
      "step=100, a1= 98.81, a2= 24.90, loss=138.94\n",
      "\n",
      "学習結果\n",
      "Estimated: a1= 98.81, a2= 24.90\n",
      "\n",
      "予測\n",
      "[[297.2274]]\n"
     ]
    }
   ],
   "source": [
    "# 0. ライブラリをロード\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. 予測式(モデル)を記述\n",
    "## 入力変数と出力変数のプレースホルダを生成\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "## モデルパラメータ \n",
    "a = tf.Variable(tf.zeros((2, 1)), name=\"a\") \n",
    "\n",
    "## モデル式 \n",
    "y = tf.matmul(x, a)\n",
    "\n",
    "# 2. 誤差関数と最適化手法を記述\n",
    "## 誤差関数(loss) \n",
    "loss = tf.reduce_mean(tf.square(y_ - y)) \n",
    "\n",
    "## 最適化手段を選ぶ(最急降下法) \n",
    "train_step = tf.train.GradientDescentOptimizer(0.02).minimize(loss)\n",
    "\n",
    "# 3.1. 訓練データを作成or読込\n",
    "train_x = np.array([[1., 3.], [3., 1.], [5., 7.]])\n",
    "train_y = np.array([190., 330., 660.]).reshape(3, 1)\n",
    "print \"\\n訓練データ\"\n",
    "print \"x=\", train_x \n",
    "print \"y=\", train_y\n",
    "\n",
    "# 3.2 学習実行\n",
    "## セッションを準備し，変数を初期化 \n",
    "sess = tf.Session() \n",
    "init = tf.initialize_all_variables() \n",
    "sess.run(init)\n",
    "\n",
    "## 最急勾配法でパラメータ更新 (100回更新する) \n",
    "print \"\\n推定パラメータの推移\"\n",
    "for i in range(100):\n",
    "    _, l, a_ = sess.run([train_step, loss, a], feed_dict={x: train_x, y_: train_y})\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print \"step=%3d, a1=%6.2f, a2=%6.2f, loss=%.2f\" % (i + 1, a_[0], a_[1], l) \n",
    "\n",
    "## 学習結果を出力 \n",
    "est_a = sess.run(a, feed_dict={x: train_x, y_: train_y}) \n",
    "print \"\\n学習結果\"\n",
    "print \"Estimated: a1=%6.2f, a2=%6.2f\" % (est_a[0], est_a[1]) \n",
    "\n",
    "\n",
    "# 4. 予測\n",
    "## (1) 新しいデータを用意\n",
    "new_x = np.array([2., 4.]).reshape(1, 2) \n",
    "\n",
    "## (2) 学習結果をつかって，予測実施 \n",
    "new_y = sess.run(y, feed_dict={x: new_x})\n",
    "print \"\\n予測\"\n",
    "print new_y \n",
    "\n",
    "# 5. 後片付け\n",
    "## セッションを閉じる \n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気まぐれおやじのこころを読む Ⅱ\n",
    "35ページ～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "訓練データ\n",
      "x= [[1. 3.]\n",
      " [3. 1.]\n",
      " [5. 7.]]\n",
      "y= [[190.]\n",
      " [330.]\n",
      " [660.]]\n",
      "\n",
      " 推定パラメータの推移\n",
      "step=100, a1=2970.66, a2=3662.66, b=984.67, loss=-11424525.00\n",
      "step=200, a1=5951.33, a2=7335.32, b=1769.34, loss=-22918782.00\n",
      "step=300, a1=8931.99, a2=11007.98, b=2554.00, loss=-34413036.00\n",
      "step=400, a1=11912.66, a2=14680.63, b=3338.67, loss=-45907296.00\n",
      "step=500, a1=14893.32, a2=18353.29, b=4123.34, loss=-57401548.00\n",
      "step=600, a1=17873.99, a2=22025.95, b=4908.01, loss=-68895808.00\n",
      "step=700, a1=20854.65, a2=25698.60, b=5692.68, loss=-80390064.00\n",
      "step=800, a1=23835.31, a2=29371.26, b=6477.34, loss=-91884320.00\n",
      "step=900, a1=26815.98, a2=33043.91, b=7262.01, loss=-103378568.00\n",
      "step=1000, a1=29796.64, a2=36716.57, b=8046.68, loss=-114872832.00\n",
      "\n",
      " 学習結果\n",
      "Estimated: a1=29796.64, a2=36716.57, b=8046.68\n",
      "\n",
      " 予測\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# sys.exit(\"準備中\")\n",
    "\n",
    "# 0. ライブラリをロード\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1. 予測式(モデル)を記述\n",
    "## 1. 学習したいモデルを記述する \n",
    "## 入力変数と出力変数のプレースホルダを生成 \n",
    "x = tf.placeholder(tf.float32, shape=(None, 2), name=\"x\") \n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\") \n",
    "\n",
    "## モデルパラメータ \n",
    "a = tf.Variable(-10 * tf.ones((2, 1)), name=\"a\") \n",
    "b = tf.Variable(200., name=\"b\") \n",
    "\n",
    "## モデル式 \n",
    "u = tf.matmul(x, a) + b \n",
    "y = tf.sigmoid(u)\n",
    "\n",
    "# 2. 誤差関数と最適化手法を記述\n",
    "## 誤差関数(loss) \n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=u, labels=y_)) \n",
    "\n",
    "## 最適化手段(最急降下法) \n",
    "train_step = tf.train.GradientDescentOptimizer(0.02).minimize(loss)\n",
    "\n",
    "# 3.1. 訓練データを作成or読込\n",
    "train_x = np.array([[1., 3.], [3., 1.], [5., 7.]])\n",
    "train_y = np.array([190., 330., 660.]).reshape(3, 1)\n",
    "print \"\\n訓練データ\"\n",
    "print \"x=\", train_x \n",
    "print \"y=\", train_y\n",
    "\n",
    "# 3.2. 学習実行\n",
    "## セッションを準備し，変数を初期化 \n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "## 最急勾配法でパラメータ更新 (1000回更新する) \n",
    "print \"\\n 推定パラメータの推移\"\n",
    "for i in range(1000):\n",
    "    _, l, a_, b_ = sess.run([train_step, loss, a, b], feed_dict={x: train_x, y_: train_y}) \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print \"step=%3d, a1=%6.2f, a2=%6.2f, b=%6.2f, loss=%.2f\" % (i + 1, a_[0], a_[1], b_, l)\n",
    "        \n",
    "## 学習結果を出力 \n",
    "est_a, est_b = sess.run([a, b], feed_dict={x: train_x, y_: train_y}) \n",
    "print \"\\n 学習結果\"\n",
    "print \"Estimated: a1=%6.2f, a2=%6.2f, b=%6.2f\" % (est_a[0], est_a[1], est_b)\n",
    "\n",
    "# 4. 予測\n",
    "## (1) 新しいデータを用意\n",
    "new_x = np.array([1., 11.]).reshape(1, 2) \n",
    "\n",
    "## (2) 学習結果をつかって，予測実施 \n",
    "new_y = sess.run(y, feed_dict={x: new_x})\n",
    "print \"\\n 予測\"\n",
    "print new_y \n",
    "\n",
    "## セッションを閉じる \n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気まぐれおやじの手書き文字を慈しむ\n",
    "48ページ～"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "準備中",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 準備中\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.exit(\"準備中\")\n",
    "\n",
    "# 1. 予測式(モデル)を記述\n",
    "## 入力変数と出力変数のプレースホルダを生成 \n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) \n",
    "\n",
    "## モデルパラメータ(入力層:784ノード, 隠れ層:100ノード, 出力層:10ノード) \n",
    "W1 = tf.Variable(tf.truncated_normal([784, 100])) \n",
    "b1 = tf.Variable(tf.zeros([100])) \n",
    "W2 = tf.Variable(tf.truncated_normal([100, 10])) \n",
    "b2 = tf.Variable(tf.zeros([10])) \n",
    "\n",
    "## モデル式 \n",
    "h = tf.sigmoid(tf.matmul(x, W1) + b1) # 入力層->隠れ層 \n",
    "u = tf.matmul(h, W2) + b2 # 隠れ層->出力層 (ロジット) \n",
    "y = tf.nn.softmax(u) # 隠れ層->出力層 (ソフトマックス後) \n",
    "\n",
    "# 2. 誤差関数と最適化手法を記述\n",
    "## 誤差関数(loss) \n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=u, logits=y_)) \n",
    "\n",
    "## 最適化手段(最急降下法)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss) \n",
    "\n",
    "## 正答率(学習には用いない) \n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) \n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# 3. 学習実行\n",
    "## MNISTデータの取得\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) \n",
    "\n",
    "## バッチ型確率的勾配降下法でパラメータ更新 \n",
    "for i in range(10000): \n",
    "    ## 訓練データから100サンプルをランダムに取得 \n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100) \n",
    "    ## 学習 \n",
    "    _, l = sess.run([train_step, loss], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print \"step=%3d, loss=%.2f\" \n",
    "\n",
    "# 4. 予測\n",
    "## (1) テスト用データを1000サンプル取得 \n",
    "new_x = mnist.test.images[0:1000]\n",
    "new_y_ = mnist.test.labels[0:1000] \n",
    "\n",
    "## (2) 予測と性能評価 \n",
    "accuracy, new_y = sess.run([acc, y], feed_dict={x:new_x , y_:new_y_ })\n",
    "print \"Accuracy (for test data): %6.2f%%\" % accuracy * 100 \n",
    "print \"True Label:\", np.argmax(new_y_[0:15,], 1) \n",
    "print \"Est Label:\", np.argmax(new_y[0:15, ], 1)\n",
    "\n",
    "## セッションを閉じる \n",
    "sess.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
